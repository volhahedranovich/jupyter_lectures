{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Alexander Fridman](http://www.rocketscience.ai) and [Volha Hedranovich](http://www.rocketscience.ai) for the Lecture Course. Source and license info is on [GitHub](https://github.com/volhahedranovich/jupyter_lectures).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\">Text preprocessing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Replacing words matching regular expressions</div>\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "            \n",
    "replacer = RegexpReplacer()\n",
    "replacer.replace(\"I should've done that thing I didn't do\")\n",
    "'I should have done that thing I did not do'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex, re.IGNORECASE), repl) for (regex, repl) in patterns]\n",
    "\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "\n",
    "\n",
    "def replace_by_regexps(text):\n",
    "    \"\"\"\n",
    "    Applies RegexpReplacer to provided text\n",
    "    :param text: an input text\n",
    "    :return: result of RegexpReplacer work\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "text = \"Let's do some NLP staff!\"\n",
    "assert replace_by_regexps(text) == 'let us do some NLP staff!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Basic cleaning</div>\n",
    "\n",
    "For simplicity let's lowercase text and replace all non word characters with space symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perfomes a basic text cleaning\n",
    "    \n",
    "    :param text: an input text\n",
    "    :return: a cleaned text\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "text = \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\"\n",
    "assert clean_text(text) == 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Tokenization</div>\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "word_tokenize(sent)\n",
    "['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using word_tokenize from NLTK\n",
    "    :param text: an input text\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "tokens = tokenize_text(sent)\n",
    "assert set(tokens) == {'ipsum', '1500s', 'the', 'since', 'text', 'been', 'ever',\n",
    "                       'has', 'industry', 'lorem', 's', 'standard', 'dummy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Removing repeated characters</div>\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "    \n",
    "replacer = RepeatReplacer()\n",
    "replacer.replace('goose')\n",
    "'goose'\n",
    "replacer.replace('looooove')\n",
    "'love'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "\n",
    "def remove_repeated_characters(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes repeated letters from tokens\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "text_tokens = ['I', 'wooooould', 'like', 'to', 'showwww', 'you',\n",
    "               'basic', 'text', 'preprocessing', 'stageeeeees']\n",
    "assert remove_repeated_characters(text_tokens) == ['I', 'would', 'like', 'to', 'show',\n",
    "                                            'you', 'basic', 'text', 'preprocesing', 'stagees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Stopwords removal</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "tokens = [t for t in tokens if t not in en_stopwords]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of tokens and words shorter than 3 chars\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: filtered tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "assert remove_stopwords(tokens) == ['lorem', 'ipsum', 'industry', 'standard',\n",
    "                                    'dummy', 'text', 'ever', 'since', '1500s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Adding n-grams</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import webtext, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))\n",
    "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]\n",
    "\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))\n",
    "[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble')]\n",
    "```\n",
    "\n",
    "### Excercise:\n",
    "1. Fetch 20newsgroups dataset\n",
    "1. Combine 1st 100 texts in a single line\n",
    "1. Lowercase and split by ' '\n",
    "1. Filter stopwords\n",
    "1. Find top 10 bigrams\n",
    "1. Find top 10 trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Spelling correction</div>\n",
    "\n",
    "\n",
    "```python\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "    \n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "    \n",
    "    \n",
    "replacer = SpellingReplacer()\n",
    "replacer.replace('cookbok')\n",
    "'cookbook'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "\n",
    "    \n",
    "def correct_spelling(text_tokens):\n",
    "    \"\"\"\n",
    "    Corrects spelling using enchant package\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "tokens = ['cookbokc', 'mother', 'fother', 'pythen']\n",
    "assert correct_spelling(tokens) == ['cookbook', 'mother', 'other', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Lemmatization</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking', 'v')\n",
    "'cook'\n",
    "lemmatizer.lemmatize('texts', 'n')\n",
    "'text'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "tokens = ['texts', 'books', 'tables', 'pythons']\n",
    "assert lemmatize(tokens) == ['text', 'book', 'table', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Stemming</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own',\n",
    " 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit',\n",
    " 'refer', 'colon', 'plot']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text_tokens):\n",
    "    \"\"\"\n",
    "    Stems provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "tokens = ['texts', 'books', 'tables', 'pythons']\n",
    "assert stem(tokens) == ['text', 'book', 'tabl', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-success\">Adding synonyms</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "synset = wordnet.synsets('dummy')[0]\n",
    "synset.lemma_names()\n",
    "['dummy', 'silent_person']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonyms(text_tokens, n_synonyms=2):\n",
    "    \"\"\"\n",
    "    Adds synonyms to tokens list\n",
    "    \n",
    "    :param text_tokens: an input tokens list\n",
    "    :param n_synonyms: count of synonyms to add\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'industry', 'standard', 'dummy', 'text', 'ever', 'since', '1500s']\n",
    "assert set(add_synonyms(tokens)) == {'industry', 'lorem', 'since',\n",
    "                                     'ever', 'of_all_time', 'ipsum',\n",
    "                                     'text', 'criterion', 'standard',\n",
    "                                     'textual_matter', 'dummy', 'silent_person',\n",
    "                                     '1500s'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\">Classifing 20 news groups dataset</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying prepropcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(X):\n",
    "    from tqdm import tqdm_notebook\n",
    "    \n",
    "    X_processed = []\n",
    "    \n",
    "    for x in tqdm_notebook(X):\n",
    "        x = replace_by_regexps(x)\n",
    "        x = clean_text(x)\n",
    "        x = tokenize_text(x)\n",
    "        x = remove_repeated_characters(x)\n",
    "        x = remove_stopwords(x)\n",
    "        # x = correct_spelling(x) # disable spelling correction because of slow work\n",
    "        x = lemmatize(x)\n",
    "        x = add_synonyms(x)\n",
    "        x = ' '.join(x)\n",
    "        X_processed.append(x)\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "X = text_preprocessing_pipeline(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('data.p', 'wb') as f:\n",
    "    pickle.dump((X, y), f)\n",
    "    \n",
    "with open('data.p', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    # TODO: your code is here\n",
    ")\n",
    "\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assesing model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
