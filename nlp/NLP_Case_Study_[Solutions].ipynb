{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Alexander Fridman](http://www.rocketscience.ai) and [Volha Hedranovich](http://www.rocketscience.ai) for the Lecture Course. Source and license info is on [GitHub](https://github.com/volhahedranovich/jupyter_lectures).</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/volha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/volha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/volha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to /home/volha/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing words matching regular expressions\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "            \n",
    "replacer = RegexpReplacer()\n",
    "replacer.replace(\"I should've done that thing I didn't do\")\n",
    "'I should have done that thing I did not do'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex, re.IGNORECASE), repl) for (regex, repl) in patterns]\n",
    "\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "\n",
    "\n",
    "def replace_by_regexps(text):\n",
    "    \"\"\"\n",
    "    Applies RegexpReplacer to provided text\n",
    "    :param text: an input text\n",
    "    :return: result of RegexpReplacer work\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def replace_by_regexps(text):\n",
    "    \"\"\"\n",
    "    Applies RegexpReplacer to provided text\n",
    "    :param text: an input text\n",
    "    :return: result of RegexpReplacer work\n",
    "    \"\"\"\n",
    "    return RegexpReplacer().replace(text)\n",
    "\n",
    "\n",
    "text = \"Let's do some NLP staff!\"\n",
    "assert replace_by_regexps(text) == 'let us do some NLP staff!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic cleaning with regexps\n",
    "\n",
    "For simplicity let's lowercase text and replace all non word characters with space symbol.\n",
    "\n",
    "TODO: link or short regexp example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perfomes a basic text cleaning\n",
    "    \n",
    "    :param text: an input text\n",
    "    :return: a cleaned text\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perfomes a basic text cleaning\n",
    "    \n",
    "    :param text: an input text\n",
    "    :return: a cleaned text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('[^\\w]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\"\n",
    "assert clean_text(text) == 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "word_tokenize(sent)\n",
    "['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using word_tokenize from NLTK\n",
    "    :param text: an input text\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using word_tokenize from NLTK\n",
    "    :param text: an input text\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return word_tokenize(text, language='english')\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "tokens = tokenize_text(sent)\n",
    "assert set(tokens) == {'ipsum', '1500s', 'the', 'since', 'text', 'been', 'ever',\n",
    "                       'has', 'industry', 'lorem', 's', 'standard', 'dummy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing repeated characters\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "    \n",
    "replacer = RepeatReplacer()\n",
    "replacer.replace('goose')\n",
    "'goose'\n",
    "replacer.replace('looooove')\n",
    "'love'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "\n",
    "def remove_repeated_characters(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes repeated letters from tokens\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def remove_repeated_characters(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes repeated letters from tokens\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: tokens list\n",
    "    \"\"\"\n",
    "    replacer = RepeatReplacer()\n",
    "    return [replacer.replace(t) for t in text_tokens]\n",
    "\n",
    "\n",
    "text_tokens = ['I', 'wooooould', 'like', 'to', 'showwww', 'you',\n",
    "               'basic', 'text', 'preprocessing', 'stageeeeees']\n",
    "assert remove_repeated_characters(text_tokens) == ['I', 'would', 'like', 'to', 'show',\n",
    "                                            'you', 'basic', 'text', 'preprocesing', 'stagees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "tokens = [t for t in tokens if t not in en_stopwords]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of tokens and words shorter than 3 chars\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: filtered tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def remove_stopwords(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of tokens and words shorter than 3 chars\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: filtered tokens list\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    return [t for t in text_tokens if t not in en_stopwords and len(t) >= 3]\n",
    "\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "assert remove_stopwords(tokens) == ['lorem', 'ipsum', 'industry', 'standard',\n",
    "                                    'dummy', 'text', 'ever', 'since', '1500s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding n-grams\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import webtext, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))\n",
    "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]\n",
    "\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))\n",
    "[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble')]\n",
    "```\n",
    "\n",
    "### Excercise:\n",
    "1. Fetch 20newsgroups dataset\n",
    "1. Combine 1st 100 texts in a single line\n",
    "1. Lowercase and split by ' '\n",
    "1. Filter stopwords\n",
    "1. Find top 10 bigrams\n",
    "1. Find top 10 trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 'the'), ('i', 'have'), ('in', 'the'), ('i', 'am')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "texts = dataset['data'][:100]\n",
    "texts_as_a_single_line = ' '.join(texts)\n",
    "words = texts_as_a_single_line.lower().split()\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction\n",
    "\n",
    "\n",
    "```python\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "    \n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "    \n",
    "    \n",
    "replacer = SpellingReplacer()\n",
    "replacer.replace('cookbok')\n",
    "'cookbook'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "\n",
    "    \n",
    "def correct_spelling(text_tokens):\n",
    "    \"\"\"\n",
    "    Corrects spelling using enchant package\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def correct_spelling(text_tokens):\n",
    "    \"\"\"\n",
    "    Corrects spelling using enchant package\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    replacer = SpellingReplacer()\n",
    "    return [replacer.replace(w) for w in text_tokens]\n",
    "\n",
    "\n",
    "tokens = ['cookbokc', 'mother', 'fother', 'pythen']\n",
    "assert correct_spelling(tokens) == ['cookbook', 'mother', 'other', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking', 'v')\n",
    "'cook'\n",
    "lemmatizer.lemmatize('texts', 'n')\n",
    "'text'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "def lemmatize(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t, 'n') for t in text_tokens]\n",
    "\n",
    "tokens = ['texts', 'books', 'tables', 'pythons']\n",
    "assert lemmatize(tokens) == ['text', 'book', 'table', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own',\n",
    " 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit',\n",
    " 'refer', 'colon', 'plot']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text_tokens):\n",
    "    \"\"\"\n",
    "    Stems provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "def stem(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in text_tokens]\n",
    "\n",
    "tokens = ['texts', 'books', 'tables', 'pythons']\n",
    "assert stem(tokens) == ['text', 'book', 'tabl', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding synonyms\n",
    "\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "synset = wordnet.synsets('dummy')[0]\n",
    "synset.lemma_names()\n",
    "['dummy', 'silent_person']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonyms(text_tokens, n_synonyms=2):\n",
    "    \"\"\"\n",
    "    Adds synonyms to tokens list\n",
    "    \n",
    "    :param text_tokens: an input tokens list\n",
    "    :param n_synonyms: count of synonyms to add\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def add_synonyms(text_tokens, n_synonyms=2):\n",
    "    \"\"\"\n",
    "    Adds synonyms to tokens list\n",
    "    \n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    from nltk.corpus import wordnet\n",
    "    \n",
    "    extended_tokens = []\n",
    "    \n",
    "    for token in text_tokens:\n",
    "        synsets = wordnet.synsets(token)\n",
    "        \n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            extended_tokens.extend(synset.lemma_names()[:n_synonyms])\n",
    "        else:\n",
    "            extended_tokens.append(token)\n",
    "            \n",
    "    return extended_tokens\n",
    "\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'industry', 'standard', 'dummy', 'text', 'ever', 'since', '1500s']\n",
    "assert set(add_synonyms(tokens)) == {'industry', 'lorem', 'since',\n",
    "                                     'ever', 'of_all_time', 'ipsum',\n",
    "                                     'text', 'criterion', 'standard',\n",
    "                                     'textual_matter', 'dummy', 'silent_person',\n",
    "                                     '1500s'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifing 20 news groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying prepropcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing_pipeline(X):\n",
    "    from tqdm import tqdm_notebook\n",
    "    \n",
    "    X_processed = []\n",
    "    \n",
    "    for x in tqdm_notebook(X):\n",
    "        x = replace_by_regexps(x)\n",
    "        x = clean_text(x)\n",
    "        x = tokenize_text(x)\n",
    "        x = remove_repeated_characters(x)\n",
    "        x = remove_stopwords(x)\n",
    "        # x = correct_spelling(x) # disable spelling correction because of slow work\n",
    "        x = lemmatize(x)\n",
    "        x = add_synonyms(x)\n",
    "        x = ' '.join(x)\n",
    "        X_processed.append(x)\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "X = text_preprocessing_pipeline(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('data.p', 'wb') as f:\n",
    "    pickle.dump((X, y), f)\n",
    "    \n",
    "with open('data.p', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tfidfvectorizer',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "          ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_dense(x): return x.todense()\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=1000),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'randomforestclassifier__n_estimators': [10, 100, 1000],\n",
    "    'randomforestclassifier__max_depth': [5, 10, 20]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_space, cv=StratifiedKFold(),\n",
    "                   verbose=8, scoring='f1_weighted', n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "d0f90eb0034c4cda9a15d89bb4d694ff": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
